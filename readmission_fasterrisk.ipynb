{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readmission Prediction with FasterRisk risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "training_data = pd.read_csv(\"data/Training_Data.csv\",index_col=0)\n",
    "testing_data = pd.read_csv(\"data/Testing_Data.csv\",index_col=0)\n",
    "# Fill nan ICU_LOS with 0 to avoid error in risk score computation\n",
    "training_data.fillna(value={'ICU_LOS':0},inplace=True)\n",
    "testing_data.fillna(value={'ICU_LOS':0},inplace=True)\n",
    "# Combine training and testing data for discretize continuous features\n",
    "all_data = pd.concat([training_data,testing_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize features with continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_discretizer(feature_name,all_data):\n",
    "    \"\"\"\n",
    "    Fits a discretizer to discretize a continuous feature in the data.\n",
    "    Returns the fitted discretizer and associated feature names.\n",
    "    \"\"\"\n",
    "    # Initialize the discretizer with 10 bins based on quantiles\n",
    "    discretizer = KBinsDiscretizer(n_bins=10,encode = 'onehot-dense',strategy='quantile',random_state=42)\n",
    "    # Fit the discretizer\n",
    "    feature_arr = all_data[feature_name].values.reshape((-1,1))\n",
    "    discretizer.fit(feature_arr)\n",
    "    # Save the fitted discretizer\n",
    "    joblib.dump(discretizer, f'data/{feature_name}_discretizer.pkl')\n",
    "    # Generate the corresponding feature names\n",
    "    bin_cutoffs = discretizer.bin_edges_[0]\n",
    "    discretized_feature_names = []\n",
    "    discretized_feature_names.append(f\"{feature_name}<{'%.2f' % (bin_cutoffs[1])}\")\n",
    "    for i in range(1,len(bin_cutoffs)-2):\n",
    "        discretized_feature_names.append(f\"{feature_name}_in_[{'%.2f' % (bin_cutoffs[i])},{'%.2f' % (bin_cutoffs[i+1])})\")\n",
    "    discretized_feature_names.append(f\"{feature_name}>={'%.2f' % (bin_cutoffs[-2])}\")\n",
    "    # Save the discretized feature names\n",
    "    joblib.dump(discretized_feature_names, f'data/{feature_name}_discretized_names.pkl')\n",
    "    return discretizer,discretized_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(discretizer,discretized_feature_names,array_to_discretize):\n",
    "    \"\"\"\n",
    "    Use a fitted discretizer to transform a continuous feature column in the data.\n",
    "    \"\"\"\n",
    "    # Discretize the feature\n",
    "    discretized_features = discretizer.transform(array_to_discretize)\n",
    "    # Create a dataframe from the discretized feature\n",
    "    discretized_df = pd.DataFrame(discretized_features,columns=discretized_feature_names)\n",
    "    return discretized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_continuous_feature(feature_name,all_data,df_to_discretize):\n",
    "    \"\"\"\n",
    "    Discretize a continuous feature in the dataframe.\n",
    "    \"\"\"\n",
    "    discretizer,discretized_feature_names = fit_discretizer(feature_name,all_data)\n",
    "    discretized_df = discretize(discretizer,discretized_feature_names,df_to_discretize[feature_name].values.reshape((-1,1)))\n",
    "    discretized_df.index = df_to_discretize.index\n",
    "    return discretized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the LOS, age, and ICU_LOS columns for the training and testing data\n",
    "LOS_train_discretized = discretize_continuous_feature('LOS',all_data,training_data)\n",
    "age_train_discretized = discretize_continuous_feature('age',all_data,training_data)\n",
    "ICU_LOS_train_discretized = discretize_continuous_feature('ICU_LOS',all_data,training_data)\n",
    "train_discretized = pd.concat([training_data,LOS_train_discretized,age_train_discretized,ICU_LOS_train_discretized],axis=1)\n",
    "LOS_test_discretized = discretize_continuous_feature('LOS',all_data,testing_data)\n",
    "age_test_discretized = discretize_continuous_feature('age',all_data,testing_data)\n",
    "ICU_LOS_test_discretized = discretize_continuous_feature('ICU_LOS',all_data,testing_data)\n",
    "test_discretized = pd.concat([testing_data,LOS_test_discretized,age_test_discretized,ICU_LOS_test_discretized],axis=1)\n",
    "# Drop the original continuous columns\n",
    "train_discretized.drop(columns=['LOS','age','ICU_LOS'],inplace=True)\n",
    "test_discretized.drop(columns=['LOS','age','ICU_LOS'],inplace=True)\n",
    "# Save the discretized test data for easy inference by the LLM assistant\n",
    "test_discretized.to_csv('data/Testing_Data_Discretized.csv',header=True,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features and labels from the discretized training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and labels\n",
    "X_train = train_discretized.drop(\"Readmission\",axis=1).astype(float).to_numpy()\n",
    "y_train = train_discretized['Readmission'].astype(float).to_numpy()\n",
    "y_train[y_train == 0] = -1\n",
    "X_test = test_discretized.drop(\"Readmission\",axis=1).astype(float).to_numpy()\n",
    "y_test = test_discretized['Readmission'].astype(float).to_numpy()\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample the minority class to balance the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority class\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  1.]), array([30497,  1676]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution before oversampling\n",
    "np.unique(y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  1.]), array([30497, 30497]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution after oversampling\n",
    "np.unique(y_train_res,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60994, 66)\n",
      "(13789, 66)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing fata shape\n",
    "print(X_train_res.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the FasterRisk risk score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a risk score model with 10 items\n",
    "optimizer = RiskScoreOptimizer(X = X_train_res, y = y_train_res, k = 10, parent_size = 10)\n",
    "optimizer.optimize()\n",
    "all_multipliers, all_intercepts, all_coefficients = optimizer.get_models()\n",
    "# Get the first model from 50 candidates \n",
    "multiplier = all_multipliers[0]\n",
    "intercept = all_intercepts[0]\n",
    "coefficients = all_coefficients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the risk score model as a readmission classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RiskScoreClassifier(multiplier, intercept, coefficients, X_train = X_train_res)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_pred_prob = classifier.predict_prob(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: [[7184 5886]\n",
      " [ 257  462]]\n",
      "AUC: 0.6247364410955026\n"
     ]
    }
   ],
   "source": [
    "# Get performance metrics\n",
    "print(\"Confusion matrix:\",confusion_matrix(y_test,y_test_pred))\n",
    "print(\"AUC:\",roc_auc_score(y_test,y_test_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save the risk score model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.                circulatory      2 point(s) |   ...\n",
      "2.                      supp1      2 point(s) | + ...\n",
      "3.                        ICU     -4 point(s) | + ...\n",
      "4.              ADM_EMERGENCY      2 point(s) | + ...\n",
      "5. ETH_BLACK/AFRICAN AMERICAN      2 point(s) | + ...\n",
      "6.          ETH_OTHER/UNKNOWN     -3 point(s) | + ...\n",
      "7.                   LOS<2.27     -2 point(s) | + ...\n",
      "8.       LOS_in_[11.57,15.61)      2 point(s) | + ...\n",
      "9.       LOS_in_[15.61,23.71)      2 point(s) | + ...\n",
      "10.                 LOS>=23.71      4 point(s) | + ...\n",
      "                                        SCORE | =    \n",
      "SCORE |  -9.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n",
      "RISK  |  11.2% |  16.1% |  19.0% |  22.4% |  26.2% |  30.4% |  35.0% |  39.8% |  44.9% |  50.0% |  55.1% |  60.2% |\n",
      "SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  16.0  |\n",
      "RISK  |  65.0% |  69.6% |  73.8% |  77.6% |  81.0% |  83.9% |  86.5% |  88.8% |  90.7% |  92.3% |  93.6% |  95.7% |\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(train_discretized.drop(\"Readmission\",axis=1).columns)\n",
    "classifier.reset_featureNames(feature_names)\n",
    "classifier.print_model_card()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/faster_risk.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(classifier, 'data/faster_risk.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
